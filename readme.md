# KhÃ³a há»c freecodecamp

## Nguá»“n
https://www.youtube.com/watch?v=mBoX_JCKZTE

## Táº¡o project scrapy

```sh
scrapy startproject web_crawler
cd web_crawler
scrapy genspider <tÃªn spider> <tÃªn domain: doanhnhansaigon.vn>

web_crawler/
â”œâ”€â”€ scrapy.cfg
â”œâ”€â”€ README.md
â”œâ”€â”€ .env
â””â”€â”€ web_crawler/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ items.py
    â”œâ”€â”€ loaders.py                  # Äá»‹nh nghÄ©a cÃ¡c item loader dá»ƒ xá»­ lÃ½ dá»¯ liá»‡u crawl trÆ°á»›c khi vÃ o loader.
    â”œâ”€â”€ middlewares/                # Viáº¿t cÃ¡c hÃ m middleware tÃ¹y chá»‰nh náº¿u cÃ³ (proxy, retry, ...)
    â”‚   â””â”€â”€ custom_proxy.py
    â”œâ”€â”€ pipelines/                  # Viáº¿t cÃ¡c pipeline xá»­ lÃ½ nÃ¢ng cao nhÆ° lÆ°u vÃ o db, xuáº¥t CSV, xá»­ lÃ½ áº£nh.
    â”‚   â””â”€â”€ export_to_minio.py
    â”œâ”€â”€ parsers/                    # folder nÃ y thÆ°á»ng Ä‘Æ°á»£c táº¡o ra Ä‘á»ƒ tÃ¡ch cÃ¡c hÃ m phÃ¢n tÃ­ch html/xml phá»©c táº¡p ra khá»i file spider chÃ­nh.
    â”‚   â””â”€â”€ product_parser.py
    â”œâ”€â”€ jobs/                       # dÃ¹ng khi muá»‘n cháº¡y folder theo kiá»ƒu checkpoint vá»›i tÃ¹y chá»n -s JOBDIR=....
    â”‚   â””â”€â”€ jobs.py
    â”œâ”€â”€ tests/                      # Viáº¿t unit test cho hÃ m xá»­ lÃ½ HTML hoáº·c logic crawl.
    â”‚   â””â”€â”€ unit.py
    â”œâ”€â”€ logs/                       # LÆ°u láº¡i log tiáº¿n trÃ¬nh
    â”‚   â””â”€â”€ app.logs
    â”œâ”€â”€ utils.py                    # Chá»©a cÃ¡c hÃ m tiá»‡n Ã­ch nhÆ°: xá»­ lÃ½ ngÃ y thÃ¡ng, lÃ m sáº¡ch dá»¯ liá»‡u, decode HTML,...
    â”œâ”€â”€ constants.py                # Khai bÃ¡o cÃ¡c háº±ng sá»‘ dÃ¹ng chung nhÆ° headers, base URLs, regex pattern,...
    â”œâ”€â”€ validators.py               # Kiá»ƒm tra tÃ­nh há»£p lá»‡ cá»§a dá»¯ liá»‡u trÆ°á»›c khi vÃ o db
    â”œâ”€â”€ settings.py
    â””â”€â”€ spiders/
        â””â”€â”€ my_spider.py

scrapy crawl <my_spider>
```
CÃ³ 2 loáº¡i middlewares: downloader middlewares (thÆ°á»ng hay Ä‘Æ°á»£c sá»­ dá»¥ng), spider middlewares. Chá»n cÃ¡i nÃ o thÃ¬ chá»‰nh trong setting.py

## Thuáº­t ngá»¯
1. feed: 1 dÃ²ng dá»¯ liá»‡u, ná»™i dung Ä‘Æ°á»£c cung cáº¥p tá»« soure vá» Ä‘Ã­ch (nhÆ° record)
    RSS feed: DÃ²ng tin tá»©c hoáº·c bÃ i bÃ¡o tá»« má»™t trang web
    Data feed: DÃ²ng dá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p tá»« má»™t há»‡ thá»‘ng (nhÆ° api hoáº·c db) cho á»©ng dá»¥ng khÃ¡c
    Feed Export: NÆ¡i dá»¯ liá»‡u Ä‘Æ°á»£c xuáº¥t ra (export). Trong scrapy há»— trá»£ feed export ra nhiá»u loáº¡i nhÆ° json, csv, ...

2. site = website

## Ká»¹ nÄƒng debug
Truy cáº­p vÃ o scrapy shell báº±ng lá»‡nh scrapy shell (cÃ³ thá»ƒ thay shell máº·c Ä‘á»‹nh báº±ng ipython - nhÆ° jupyter)
Náº¿u dÃ¹ng scrapy redis thÃ¬ khi debug cáº§n bá» Ä‘oáº¡n item pipeline Ä‘i Ä‘á»ƒ cÃ³ thá»ƒ cÃ o tá»‘t hÆ¡n.

## Lá»‡nh
1. scrapy list: liá»‡t kÃª cÃ¡c spider
2. scrapy crawl <tÃªn spider>: cháº¡y 1 con nhá»‡n
    option lÆ°u dá»¯ liá»‡u: 
    - thÃªm -O <tÃªn file><extention> Ä‘á»ƒ lÆ°u dá»¯ liá»‡u vÃ o 1 file nÃ o Ä‘Ã³, sáº½ ghi Ä‘Ã¨ náº¿u file Ä‘Ã£ tá»“n táº¡i
    - thÃªm -o <tÃªn file><extention> tÃ¡c dá»¥ng tÆ°Æ¡ng tá»± nhÆ°ng append vÃ o file náº¿u nÃ³ Ä‘Ã£ tá»“n táº¡i
    - sá»­ dá»¥ng file setting.py

## 8. Fake User-Agent and Brower headers
Má»¥c Ä‘Ã­ch: lÃ m cho con bot giá»‘ng nhÆ° Ä‘ang sá»­ dá»¥ng trÃ¬nh duyá»‡t tháº­t
User-agent: Chá»©a thÃ´ng tin cá»§a trÃ¬nh duyá»‡t, cáº§n pháº£i fake string nÃ y trong header cá»§a request Ä‘á»ƒ giáº£ dáº¡ng trÃ¬nh duyá»‡t tháº­t. 
1. Sá»­ dá»¥ng 1 trÃ¬nh duyá»‡t vá»›i má»i request (khÃ´ng khuyáº¿n khÃ­ch vÃ¬ cÃ³ thá»ƒ bá»‹ cháº·n)
    - Copy User-Agent String trong tab network: Báº­t F12 -> Network tab -> Refesh láº¡i trang Ä‘á»ƒ xem nhá»¯ng gÃ¬ mÃ  trÃ¬nh duyá»‡t Ä‘Ã£ request Ä‘áº¿n server vÃ  response cá»§a nÃ³, Ä‘á»c header vÃ  láº¥y
    - Äá»ƒ Ä‘á»c Ä‘Æ°á»£c thÃ´ng tin tá»« chuá»—i, truy cáº­p trang web [sau](https://useragentstring.com/) vÃ  Ä‘iá»n User-Agent vÃ o
2. Fake 1 user-agent má»¡i má»—i request
    - Ã tÆ°á»Ÿng táº¡o 1 list user/agent vÃ  loop qua nÃ³
    - Vá»›i cÃ¡c trang web lá»›n, cáº§n pháº£i táº¡o list cÃ³ hÃ ng ngÃ n user-agent. Hoáº·c sÆ° dá»¥ng Fake User Agent API (chá»‰ Ä‘á»‹nh trong file middlewares.py)
    - [API generator User-Agent](https://scrapeops.io/?fpr=lucas37&gad_source=1&gclid=CjwKCAiAzPy8BhBoEiwAbnM9O-yfrcSfS-0rgtjGz8fmFBTEUaem4rH0MVKA6jfvCX9qKh5MLY7j6hoCpD0QAvD_BwE): Cáº§n pháº£i táº¡o tÃ i khoáº£n (vohoang.w2002), scrapy cung cáº¥p free
3. Fake toÃ n bá»™ header
    - Fake user-agent lÃ  khÃ´ng Ä‘á»§ vá»›i cÃ¡c trang web lá»›n nhÆ° google, amazon, ... cáº§n pháº£i fake toÃ n bá»™ request header cho phÃ¹ há»£p vá»›i broswer.
    - Sá»­ dá»¥ng link api trÃªn cÃ³ thá»ƒ lÃ m Ä‘Æ°á»£c

## 9. Proxy
0. CÃ¡c trang web proxy free 
    - https://geonode.com/free-proxy-list
    - https://free-proxy-list.net/
1. Cho má»¥c Ä‘Ã­ch há»c táº­p, táº¡o mÃ¡y áº£o:
    - Forward Proxy (proxy thuáº­n - Ä‘áº¡i diá»‡n cho client): lÃ  mÃ¡y chá»§ trung gian giá»¯a client vÃ  internet. CÃ³ trÃ¡ch nhiá»‡m thay máº·t client gá»­i request Ä‘áº¿n báº¥t ká»³ Ä‘Ã¢u trong internet (router bgp cháº³ng háº¡n hay vpn). TÃ¡c dá»¥ng, che dáº¥Ãº ip cá»§a client (t,hÆ°á»ng ip client lÃ  private IP thÃ´ng qua bgp nÃªn cÅ©ng cháº³ng cáº§n che dáº¥u :v), kiá»ƒm soÃ¡t truy cáº­p, lá»c ná»™i dung, caching tÄƒng tá»‘c Ä‘á»™ truy cáº­p.
    ```sh
        sudo apt install squid -y
        sudo vi /etc/squid/squid.conf
        
        # acl localnet src 192.168.56.0/24   (cho phÃ©p cÃ¡c mÃ¡y thuá»™c dáº£i máº¡ng trÃªn sá»­ dá»¥ng proxy)
        # http_access allow localnet
        sudo systemctl restart squid

        # Test local, máº·c Ä‘á»‹nh squid chá»‰ há»— trá»£ http, cáº¥u hÃ¬nh https pháº£i thiáº¿t láº­p thÃªm
        curl -x http://192.168.56.161:3128 -I https://facebook.com
    ```
    - Reverse Proxy (proxy ngÆ°á»£c - Ä‘áº¡i diá»‡n cho server): lÃ  mÃ¡y chá»§ trung gian trong trÃ¬nh duyá»‡t web. Náº±m giá»¯a client vÃ  cÃ¡c server backend (che dáº¥u Ä‘Æ°á»£c ip server backend) vÃ  chuyá»ƒn tiáº¿p request cá»§a client Ä‘áº¿n server backend phÃ¹ há»£p (cÃ¢n báº±ng táº£i giá»¯a cÃ¡c server backend). TÃ¡c dá»¥ng cÃ¢n báº±ng táº£i, báº£o máº­t, caching, ssl termination (giáº£i mÃ£ SSL/TTS thay cho server backend).
    - Residential Proxies: 
2. Uptime lÃ  thá»i gian mÃ  má»™t dá»‹ch vá»¥ (nhÆ° proxy) hoáº¡t Ä‘á»™ng liÃªn tá»¥c mÃ  khÃ´ng bá»‹ giÃ¡n Ä‘oáº¡n. VÃ­ dá»¥, náº¿u má»™t proxy cÃ³ uptime 99%, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  trong 100 giá», proxy Ä‘Ã³ cÃ³ thá»ƒ sáº½ ngá»«ng hoáº¡t Ä‘á»™ng 1 giá». Uptime cÃ ng cao thÃ¬ proxy cÃ ng Ä‘Ã¡ng tin cáº­y vÃ  Ã­t bá»‹ giÃ¡n Ä‘oáº¡n.
3. Sá»­ dá»¥ng project cá»§a scrapy
    pip install scrapy-rotating-proxies
4. PhÆ°Æ¡ng phÃ¡p 1: Liá»‡t kÃª cÃ¡c proxy vÃ o file setting.py
5. PhÆ°Æ¡ng phÃ¡p 2: Liá»‡t kÃª cÃ¡c proxy vÃ o file proxies.txt vÃ  thÃªm Ä‘Æ°á»ng dáº«n cá»§a Ä‘áº¿n file Ä‘Ã³ vÃ o file setting.py
6. PhÆ°Æ¡ng phÃ¡p 3: Sá»­ dá»¥ng service cung cáº¥p ip vÃ  port cá»§a proxy hoáº¡t Ä‘á»™ng
    - 2 phÆ°Æ¡ng phÃ¡p trÃªn Ä‘ÃªÃ¹ cÃ³ nhÆ°á»£c Ä‘iá»ƒm lÃ  scrapy pháº£i check tá»«ng proxies hoáº¡t Ä‘á»™ng hay ko. LÃºc nÃ o cÅ©ng pháº£i lo láº¯ng vá» danh sÃ¡ch cÃ¡c proxies xem nÃ³ cÃ³ bá»‹ Ä‘á»•i ip hay ko, ...
    - service nÃ y Ä‘Æ°á»£c cung cáº¥p bá»Ÿi proxy provider vÃ  nÃ³ máº¥t phÃ­
    - Khi mua cáº§n pháº£i chÃº Ã½ location. Háº§u háº¿t cÃ¡c trang web sáº½ hoáº¡t Ä‘á»™ng khÃ¡c nhau khi á»Ÿ location khÃ¡c nhau.

Äi xa hÆ¡n, sá»­ dá»¥ng service cÃ³ phÃ­ cá»§a scrape ops, proxy tá»± Ä‘á»™ng cung cáº¥p dá»¯ liá»‡u trang web mÃ  ko cáº§n pháº£i fake user-agent hay sá»­ dá»¥ng proxy bÃªn thá»© 3 ná»¯a. Váº«n tÃ i khoáº£n Ä‘Ã³ trong scrape ops, Ä‘i vÃ o pháº§n proxy vÃ  xem video tá»«

## 10. Run spiders in cloud with scrapyd
1. Scrapyd - Free open source tool Ä‘á»ƒ cháº¡y nhiá»u spider trÃªn má»™t mÃ¡y chá»§ tá»« xa. 
    - LÃ  1 thÆ° viá»‡n python, cáº§n server tá»± táº¡o Ä‘á»ƒ cháº¡y
    - Chá»‰ cung cáº¥p api Ä‘á»ƒ thao tÃ¡c, giao diá»‡n cÃ³ thá»ƒ tÃ¹y chá»‰nh thÃ´ng qua pháº§n má»m cá»§a bÃªn thá»© 3.
    - no scheduler
    - CÃ¡ch sá»­ dá»¥ng
    ```sh
    # CÃ i Ä‘áº·t
    pip install scrapyd -y
    # Cháº¡y scrapyd á»Ÿ cháº¿ Ä‘á»™ backgroud
    scrapyd > scrapyd.log 2>&1 &
    # DÃ¹ng trÃ¬nh duyá»‡t hoáº·c curl Ä‘á»ƒ check
    curl http://localhost:6800/daemonstatus.json
    # CÃ i scrapyd client cho mÃ¡y chá»©a project vÃ  up nÃ³ lÃªn server.
    pip install git+https://github.com/scrapy/scrapyd-client.git
    # Äiá»u chá»‰nh trong file scrapy.cfg Ä‘á»ƒ chá»©a url cá»§a server (default lÃ  project name trong scrapyd)
    scrapy-deploy default
    # Project sáº½ tá»± sinh ra cÃ¡c thÆ° má»¥c nhÆ° build, project.egg-info
    # BÃ¢y giá» project Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai thÃ nh cÃ´ng lÃªn scrapy vÃ  sáºµn sÃ ng Ä‘á»ƒ cháº¡y
    curl http://localhost:6800/schedule.json -d project=bookscraper -d spider=bookspider
    # Dá»«ng scrapyd
    sudo ss -tunlp (láº¥y pid vÃ  kill nÃ³)
    kill <pid>
    ```

2. ScrapeOps - Free
    - Cáº§n cÃ³ server riÃªng tá»± táº¡o vÃ  cÃ i scrapyd
    - CÃ³ UI
    - CÃ³ kháº£ nÄƒng táº¡o jobs/spider vÃ  mornitor

3. ScrapyCloud - Paid

Triá»ƒn khai scrapyd
```sh
# TrÃªn mÃ¡y server scrapyd
git clone --depth 1 https://github.com/nkt780426/Learn-Scrapy.git
sudo apt install python3.10-venv -y
python3 -m venv crawl-data
source crawl-data/bin/activate
cd cd Learn-Scrapy/
pip install -r requirements.txt

cd Learn-Scrapy/crawl-data/lib/python3.10/site-packages/scrapyd/
vi default_scrapyd.conf # Sá»­a bind_address thÃ nh 0.0.0.0 Ä‘á»ƒ cháº¥p nháº­n client tá»« má»i nÆ¡i

# Cháº¡y scrapyd á»Ÿ cháº¿ Ä‘á»™ background vÃ  ghi log vÃ o file scrapyd.log thay vÃ¬ in ra terminal
export SCRAPYD_BIND_ADDRESS=0.0.0.0
scrapyd > scrapyd.log 2>&1 &

# Kiá»ƒm tra status cá»§a scrapyd server tá»« mÃ¡y client
(base) user3t@LAP027:~$ curl http://192.168.56.160:6800/daemonstatus.json
{"pending": 0, "running": 0, "finished": 0, "status": "ok", "node_name": "scrapyd"} # KhÃ´ng cÃ³ job nÃ o pedding, running, finished

# Package spider vÃ  up lÃªn srapyd server
# Chá»‰nh url cá»§a server trong file scrapy.cfg
# Theo máº·c Ä‘á»‹nh scrapy chá»‰ Ä‘Ã³ng gÃ³i cÃ¡c tá»‡p scrapy lÃªn server. Muá»‘n nÃ³ cÃ³ thÃªm file proxies.txt cáº§n pháº£i config trong setup.py
# Push spider lÃªn server (nhÆ° git)
scrapyd-deploy default
(craw_data) (base) user3t@LAP027:~/Workspace/projects/in-process/scrapy/bookscraper$ scrapyd-deploy default
Packing version 1738723310
Deploying to project "bookscraper" in http://192.168.56.160:6800/addversion.json
Server response (200):
{"project": "bookscraper", "version": "1738723310", "spiders": 1, "status": "ok", "node_name": "scrapyd"} # Project name, version push, sá»‘ lÆ°á»£ng spider trong project, status push, hostnamectl cá»§a server.
# Scheduler spider
curl http://192.168.56.160:6800/schedule.json -d project=bookscraper -d spider=bookspider
# Kiá»ƒm tra job cÃ³ cháº¡y ko
curl http://192.168.56.160:6800/listjobs.json?project=bookscraper
# Kiá»ƒm tra log
curl http://192.168.56.160:6800/logs/bookscraper/bookspider/af830432e36c11efaf13a10e0aa0ee42.log
```
Triá»ƒn khai scrapydweb
```sh
pip install -r requirements.txt
# Láº§n Ä‘áº§u tiÃªn cháº¡y sáº½ sinh ra file scrapydweb_settings_v10.py á»Ÿ ~
scrapydweb
# Äiá»u chá»‰nh cáº¥u hÃ¬nh thÃ nh
SCRAPYD_SERVERS = [
    '127.0.0.1:6800'
]
ENABLE_LOGPARSER = True 
LOCAL_SCRAPYD_SERVER = '127.0.0.1:6800'
LOCAL_SCRAPYD_LOGS_DIR = '/home/vohoang/scrapyd/logs'
# Dá»«ng scrapyd vÃ  start láº¡i
mkdir scrapyd
cd scrapyd 
scrapyd > scrapyd.log 2>&1 &
# Cháº¡y láº¡i vÃ  táº­n hÆ°á»Ÿng á»Ÿ port 5000 (flask)
scrapydweb > scrapydweb.log 2>&1 &
```

## 13. Recap
1. CÃ¡c váº¥n Ä‘á» cÃ²n tá»“n Ä‘á»ng
    - dynamic website: ná»™i dung sinh ra chá»‰ khi di chuyá»ƒn view, front-end framework chá»‰ hiá»ƒn thá»‹ 1 pháº§n thÃ´ng tin (1 page) mÃ  server gá»­i. Do Ä‘Ã³ náº¿u sá»­ dá»¥ng url tá»« website nÃ y sáº½ khÃ´ng nháº­n Ä‘Æ°á»£c data. => Scrapy pupeteer hoáº·c scrapy selenium
```sh
# dÃ¹ng Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u tá»« cÃ¡c trang web sá»­ dá»¥ng JavaScript Ä‘á»™ng nhÆ° SPA - Single Page Application. Puppeteer giÃºp render hoÃ n chá»‰nh trang web giÃºp scrapy láº¥y Ä‘Æ°á»£c dá»¯ liá»‡u sau khi JS cháº¡y xong
pip install scrapy-puppeteer
# ThÃªm middleware vÃ o puppeteer
DOWNLOADER_MIDDLEWARES = {
    'scrapy_puppeteer.middleware.PuppeteerMiddleware': 800,
}
# Táº¡o spider
import scrapy
from scrapy_puppeteer import PuppeteerRequest

class MySpider(scrapy.Spider):
    name = 'myspider'
    start_urls = ['https://example.com']

    async def parse(self, response):
        yield PuppeteerRequest(
            url='https://example.com',
            callback=self.parse_result
        )

    async def parse_result(self, response):
        title = response.css('title::text').get()
        yield {'title': title}
# DÃ¹ng khi trang web sá»­ dá»¥ng nhiá»u js Ä‘á»ƒ táº£i ná»™i dung hay ko thá»ƒ láº¥y Ä‘Æ°á»£c dá»¯ liá»‡u vÃ¬ ná»™i dung khÃ´ng cÃ³ trong html ban Ä‘áº§u, cáº§n tÆ°Æ¡ng tÃ¡c vá»›i trang web nhÆ° click, scroll, Ä‘iá»n form, ...
# Náº¿u chá»‰ cáº§n láº¥y dá»¯ liá»‡u tá»« API áº©n trong trang thÃ¬ cÃ³ thá»ƒ thá»­ Scrapy + request vÃ¬ puppeteer cháº­m hÆ¡n.
```
```sh
# scrapy-selenium cÅ©ng lÃ  middleware há»— trá»£ crawl dynamic web
# selenium sá»­ dá»¥ng webdriver thay vÃ¬ puppeteer
# DÃ¹ng khi cáº§n tÆ°Æ¡ng tÃ¡c vá»›i web nhÆ° click, scroll, Ä‘iá»n form, Ä‘Äƒng nháº­p, ...
pip install scrapy-selenium
# CÃ i thÃªm web driver ná»¯a
DOWNLOADER_MIDDLEWARES = {
    'scrapy_selenium.SeleniumMiddleware': 800
}

SELENIUM_DRIVER_NAME = 'chrome'  # Hoáº·c 'firefox'
SELENIUM_DRIVER_EXECUTABLE_PATH = '/path/to/chromedriver'  # ÄÆ°á»ng dáº«n Ä‘áº¿n WebDriver
SELENIUM_DRIVER_ARGUMENTS=['--headless']  # Cháº¡y khÃ´ng hiá»ƒn thá»‹ trÃ¬nh duyá»‡t
# VÃ­ dá»¥ Ä‘Æ¡n giáº£n
import scrapy
from scrapy_selenium import SeleniumRequest

class MySpider(scrapy.Spider):
    name = "my_spider"

    def start_requests(self):
        yield SeleniumRequest(
            url="https://example.com",
            callback=self.parse
        )

    def parse(self, response):
        title = response.css('title::text').get()
        yield {"title": title}
```

| ğŸ›  CÃ´ng cá»¥         | ğŸŒ TrÃ¬nh duyá»‡t                  | ğŸš€ Hiá»‡u suáº¥t  | ğŸ”§ Khi nÃ o nÃªn dÃ¹ng?                                      |
|-------------------|--------------------------------|--------------|------------------------------------------------------|
| **Scrapy-Selenium**  | Chrome, Firefox, Edge, Safari... | Cháº­m hÆ¡n     | Khi cáº§n tÆ°Æ¡ng tÃ¡c vá»›i trang web (click, form...)     |
| **Scrapy-Puppeteer** | Chá»‰ há»— trá»£ Chrome/Chromium    | Nhanh hÆ¡n    | Khi chá»‰ cáº§n render JavaScript mÃ  khÃ´ng cáº§n nhiá»u tÆ°Æ¡ng tÃ¡c |

Sá»­ dá»¥ng 2 cÃ´ng cá»¥ trÃªn sáº½ cháº¡y vá»›i headless broswer (khÃ´ng cáº§n ui)

2. Login endpoint
3. Scale scrape (náº¿u pháº£i cÃ o 1000 page 1 ngÃ y thÃ¬ sao)
    - Sá»­ dá»¥ng database Ä‘á»ƒ lÆ°u cÃ¡c url cáº§n xá»­ lÃ½, sau Ä‘Ã³ dÃ¹ng nhiá»u server

Äá»c link mÃ  khÃ³a há»c recommend. scrapy-playwright Ä‘Ã£ thay tháº¿ scrapy-puppeteer
https://thepythonscrapyplaybook.com/freecodecamp-beginner-course/freecodecamp-scrapy-beginners-course-part-13-next-steps/

# KhÃ³a há»c beginer

## Nguá»“n:
https://www.youtube.com/watch?v=jsQQ2GyFyCg&list=PLkhQp3-EGsIhELaJyhzgLt8CsDywwHfuz&index=3

```sh
git clone --depth 1 https://github.com/ScrapeOps/python-scrapy-playbook.git
```

## Itemloader
- pipeline.py: thÆ°á»ng dÃ¹ng Ä‘á»ƒ lÆ°u item vÃ o trong DB, kiá»ƒm tra trung láº·p item. Äá»ƒ tiá»n xá»­ lÃ½ dá»¯ liá»‡u sau khi Ä‘Ã£ táº¡o item (nhÆ° Ä‘á»•i yÃªn sang vnd)
- itemloader.py: tiá»n xá»­ lÃ½ dá»¯ liá»‡u trÆ°á»›c khi táº¡o item
- item.py: chuáº©n hÃ³a dá»¯ liá»‡u

## Fake User-agent
KhÃ´ng cáº§n dÃ¹ng api Ä‘á»ƒ call ná»¯a. DÃ¹ng cÃ¡i nÃ y
```sh
pip install scrapy-user-agents
# ThÃªm vÃ o middleware
# 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
# 'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,
```

## Concurently request vÃ  Download Delay
```sh
# Sá»‘ lÆ°á»£ng request mÃ  scrapy cÃ³ thá»ƒ gá»­i cÃ¹ng 1 lÃºc Ä‘áº¿n website má»¥c tiÃªu, máº·c Ä‘á»‹nh lÃ  16, sá»­a nÃ³ trong setting.py
CONCURRENT_REQUESTS = 16
# Giá»›i háº¡n concurrent requests cho má»—i domain vÃ  ip
CONCURRENT_REQUESTS_PER_DOMAIN = 8  # Sá»‘ request tá»‘i Ä‘a cho má»—i domainm trÃ¡nh bá»‹ block khi website cÃ³ giá»›i háº¡n request tá»« cÃ¹ng 1 domain.
CONCURRENT_REQUESTS_PER_IP = 4      # Sá»‘ request tá»‘i Ä‘a cho má»—i IP, há»¯u Ã­ch khi cÃ³ nhiá»u proxy vÃ  muá»‘n kiáº¿m soÃ¡t táº£i

# Káº¿t há»£p vá»›i DOWNLOAD_DELAY Ä‘á»ƒ giáº£m thiá»ƒu bá»‹ cháº·n náº¿u ko muá»‘n bá»‹ cháº·n khi gá»­i nhiá»u request cÃ¹ng 1 lÃºc, giáº£m Ä‘á»™ trá»… giá»¯a cÃ¡c request
DOWNLOAD_DELAY = 1
# Hoáº·c káº¿t há»£p vá»›i Random delay dá»±a trÃªn tá»‘c Ä‘á»™ pháº£n há»“i cá»§a server
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 1    # Báº¯t Ä‘áº§u vá»›i 1 giÃ¢y delay
AUTOTHROTTLE_MAX_DELAY = 3      # Delay tá»‘i Ä‘a lÃ  3 giÃ¢y
AUTOTHROTTLE_TARGET_CONCURRENCY = 2  # Giá»¯ má»©c 2 request Ä‘á»“ng thá»i
```

## CÃ¡c extention hay
1. scrapy deltafetch
    - Äáº£m báº£o má»—i url chá»‰ Ä‘Æ°á»£c crawl 1 láº§n duy nháº¥t, sá»­ dá»¥ng SQLite Ä‘á»ƒ lÆ°u danh sÃ¡ch cÃ¡c url Ä‘Ã£ crawl Ä‘Æ°á»£c. File náº±m trong ./scrapy/deltafetch. Báº±ng cÃ¡ch nÃ y sáº½ giáº£m táº£i lÆ°á»£ng request Ä‘Ã£ crawl.
    - Scrapy DeltaFetch lÃ  má»™t middleware trong Scrapy giÃºp trÃ¡nh crawl láº¡i dá»¯ liá»‡u cÅ© mÃ  Ä‘Ã£ Ä‘Æ°á»£c thu tháº­p trong láº§n cháº¡y trÆ°á»›c Ä‘Ã³. NÃ³ táº¡o ra má»™t "delta crawl", chá»‰ thu tháº­p ná»™i dung má»›i trÃªn trang web.
    ```sh
    SPIDER_MIDDLEWARES = {
        'scrapy_deltafetch.DeltaFetch': 100,
    }
    DELTAFETCH_ENABLED = True  # Báº­t cháº¿ Ä‘á»™ DeltaFetch
    ```
2. Parse
    ```sh
    # Price Parser
    https://github.com/scrapinghub/price-parser/

    # Date Parser
    [dateparser](https://github.com/scrapinghub/dateparser) is a great little library that allows you to easily parse human readable dates into Python datatimes. 

        >>> import dateparser

        >>> dateparser.parse('Fri, 12 Dec 2014 10:55:50')
        datetime.datetime(2014, 12, 12, 10, 55, 50)

        >>> dateparser.parse('1991-05-17')
        datetime.datetime(1991, 5, 17, 0, 0)

        >>> dateparser.parse('In two months')  # today is 1st Aug 2020
        datetime.datetime(2020, 10, 1, 11, 12, 27, 764201)

        >>> dateparser.parse('1484823450')  # timestamp
        datetime.datetime(2017, 1, 19, 10, 57, 30)

    It supports almost every existing date format: absolute dates, relative dates (`"two weeks ago"` or `"tomorrow"`), timestamps, etc. from over 200 different languages.

    # Number-Parser
    [Number-Parser](https://github.com/scrapinghub/number-parser) is a simple library that converts numbers written in natural language into its numeric form. Examples:

        ## Parse Numbers in Place
        >>> from number_parser import parse
        >>> parse("I have two hats and thirty seven coats")
        'I have 2 hats and 37 coats'

        ## Parse Number
        from number_parser import parse_number
        >>> parse_number("two thousand and twenty")
        2020

        ## Parse Fraction
        >>> from number_parser import parse_fraction
        >>> parse_fraction("forty two divided by five hundred and six")
        '42/506'

    It currently supports cardinal numbers in the following languages - English, Hindi, Spanish and Russian and ordinal numbers in English.
    ```

# KhÃ³a há»c Scrapy-Playwright

## Nguá»“n
- Video: https://www.youtube.com/watch?v=EijzO7n2-dg
- Doc: https://scrapeops.io/python-scrapy-playbook/scrapy-playwright/

## Váº¥n Ä‘á»
- Scrapy thÆ°á»ng chá»‰ crawl Ä‘Æ°á»£c static website
- playwright lÃ  headless broswer (broswer ko cÃ³ ui) Ä‘Æ°á»£c phÃ¡t hÃ nh bá»Ÿi microsoft vÃ o 2020 vÃ  nhanh chÃ³ng trá»Ÿ thÃ nh headless broswer phá»• biáº¿n nháº¥t há»— trá»£ nhiá»u nhÃ¢n nhÆ° chromium, webkit, firefox broswer
- playwright thay tháº¿ puppeteer (puppeteer chá»‰ há»— trá»£ má»—i chromium)
- JS Render lÃ  quÃ¡ trÃ¬nh táº£i vÃ  thá»±c thi JavaScript trÃªn má»™t trang web Ä‘á»ƒ láº¥y dá»¯ liá»‡u sau khi trang Ä‘Ã£ hoÃ n táº¥t render.
    - Táº¡i sao cáº§n ?: 1 sá»‘ trang web khÃ´ng hiá»ƒn thá»‹ toÃ n bá»™ trang web lÃºc Ä‘áº§u mÃ  sá»­ dá»¥ng JavaScript Ä‘á»ƒ táº£i ná»™i dung Ä‘á»™ng nhÆ° AJAX, React, Vue, Angular, ... Scrapy chá»‰ láº¥y dá»¯ liá»‡u html mÃ  ko thá»±c thi file .js :)). Náº¿u ná»™i dung cáº§n láº¥y lÃ  js sáº£n sinh ra sau 1 thá»i gian thÃ¬ scrapy chá»‹u. scrapy-playwright giÃºp Scrapy cÃ³ thá»ƒ render JavaScript báº±ng cÃ¡ch Ä‘iá»u hÆ°á»›ng trang qua Playwright (má»™t thÆ° viá»‡n há»— trá»£ Ä‘iá»u khiá»ƒn trÃ¬nh duyá»‡t tá»± Ä‘á»™ng).

## Base project

```sh
git clone --depth 1 https://github.com/python-scrapy-playbook/quotes-js-project.git
pip install scrapy-playwright
# Download cÃ¡c web driver (lÃ m 1 láº§n duy nháº¥t)
playwright install
# CÃ³ cáº£nh bÃ¡o thiáº¿u package cáº§n thiáº¿t thÃ¬ cÃ i vÃ  kiá»ƒm tra version, khÃ´ng cÃ³ lá»—i lÃ  dÃ¹ng Ä‘Æ°á»£c (lÃ m 1 láº§n duy nháº¥t)
playwright install-deps
playwright --version
```

## Thá»±c hÃ nh
scrapy crawl quotes -O results.csv
1. Váº¥n Ä‘á» 1: trang web mÃ  ná»™i dung cá»§a nÃ³ Ä‘Æ°á»£c sinh ra bá»Ÿi javascript sau 1 thá»i gian
    - Trang web crawl: https://quotes.toscrape.com/js
        - Báº­t F12->Network refresh láº¡i trang vÃ  Ä‘á»c ná»™i dung file html (js/) sáº½ tháº¥y nÃ³ cháº³ng cÃ³ j cáº£ so vá»›i trang web tháº­t.
        - Äiá»u nÃ y lÃ  do táº¥t cáº£ quotes mÃ  trang web tháº­t cÃ³ Ä‘Æ°á»£c lÃ  js sinh ra.
    - Sá»­ dá»¥ng: chá»‰ cáº§n thÃªm meta={'playwright': True} vÃ o Scrapy request. Scrapy sáº½ dÃ¹ng playwright Ä‘á»ƒ táº£i trang nhÆ° má»™t trÃ¬nh duyá»‡t thá»±c sá»±. Tuy nhiÃªn, Ä‘Ã´i khi playwright sáº½ káº¿t thÃºc viá»‡c render trÆ°á»›c khi page Ä‘Æ°á»£c renders toÃ n bá»™ (Ä‘iá»u nÃ y Ä‘áº·c biá»‡t Ä‘Ãºng khi page cá»±c dÃ i). Cáº§n trÃ¡nh Ä‘iá»u nÃ y báº±ng cÃ¡ch thÃªm cÃ¡c tham sá»‘ khÃ¡c. ÄÃ³ng page playwright láº¡i khi khÃ´ng cáº§n thiáº¿t Ä‘á»ƒ giáº£i phÃ³ng tÃ i nguyÃªn.
    - Chá»‰ cáº§n Ä‘á»c file spider
2. Váº¥n Ä‘á» 2: trang web mÃ  scroll vÃ´ háº¡n nhÆ° facebook, linkedin, et cetera, ...
    - Trang web crawl: https://quotes.toscrape.com/scroll
    - Báº£n cháº¥t: khi scroll xuá»‘ng cuá»‘i trang, nÃ³ sáº½ thá»±c hiá»‡n 1 request Ä‘áº¿n server Ä‘á»ƒ láº¥y ná»™i dung má»›i. QuÃ¡ trÃ¬nh naá»³ láº·p láº¡i vÃ´ háº¡n.
    - Chá»‰ cáº§n Ä‘á»c file spider
3. Váº¥n Ä‘á» 3: Chá»¥p mÃ n hÃ¬nh
    - playwright cÃ³ thá»ƒ chá»¥p mÃ n hÃ¬nh page vÃ  lÆ°u :))
    - Chá»‰ cáº§n Ä‘á»c file spider

# KhÃ³a há»c scrapy-selenium, scrapy-splash

KhÃ´ng dÃ¹ng selenium ná»¯a, playwright máº¡nh hÆ¡n. Tuy nhiÃªn nÃªn biáº¿t Ä‘á»ƒ trÃ¡nh bá»‹ há»i khi phá»ng váº¥n.

- Há»— trá»£ nhiá»u loáº¡i trÃ¬nh duyá»‡t mÃ  khÃ´ng cáº§n cáº¥u hÃ¬nh driver nhÆ° selenium
- Hoáº¡t Ä‘á»™ng nhanh hÆ¡n, Ã­t tiÃªu tá»‘n tÃ i nguyÃªn hÆ¡n selenium do tÆ°Æ¡ng tÃ¡c trá»±c tiáº¿p vá»›i trang web chá»© khÃ´ng qua driver.
- HÃ´ trá»£ tÆ°Æ¡ng tÃ¡c tá»‘t hÆ¡n vá»›i trang web cÃ³ ná»™i dung JS Ä‘á»™ng mÃ  ko cáº§n viáº¿t nhiá»u code. Trong khi selenium thÃ¬ pháº£i viáº¿t ráº¥t nhiá»u code tÆ°Æ¡ng tÃ¡c vÃ  web cÃ³ nhiá»u js Ä‘á»™ng.
- Há»— trá»£ cháº¡y song song nhiá»u phiÃªn trÃ¬nh duyá»‡t má»™t cÃ¡ch dá»… dÃ ng, giÃºp tÄƒng tá»‘c Ä‘á»™ kiá»ƒm tra vÃ  tá»± Ä‘á»™ng hÃ³a. Selenium cáº§n cáº¥u hÃ¬nh phá»©c táº¡p hÆ¡n.
- Cung cáº¥p cÃ¡c API máº¡nh máº½ Ä‘á»ƒ kiá»ƒm tra táº¥t cáº£ cÃ¡c tÃ­nh nÄƒng cá»§a trÃ¬nh duyá»‡t nhÆ° chuá»™t, bÃ n phÃ­m, ...

KhÃ´ng dÃ¹ng splash, nÃ³ khÃ´ng phÃ¹ há»£p Ä‘á»ƒ xá»­ lÃ½ trang web cÃ³ js phá»©c táº¡p nhÆ° click, scroll, Ä‘iá»n form. Tuy nhiÃªn Ä‘Ã¢y láº¡i lÃ  headless broser duy nháº¥t Ä‘Æ°á»£c sinh ra chá»‰ Ä‘á»ƒ cÃ o web, nÃ³ dá»… sá»­ dá»¥ng hÆ¡n. Hiá»ƒu nÃ´m na, cáº§n dá»±ng 1 server splash (docker image cÃ³ sáºµn). Thay vÃ¬ gá»­i request trá»±c tiáº¿p Ä‘áº¿n trang thÃ¬ mk sáº½ gá»­i Ä‘áº¿n nÃ³. NÃ³ sáº½ tá»± render trang web toÃ n bá»™ vÃ  gá»­i láº¡i.

# KhÃ³a há»c scrapy login

## Nguá»“n:
- Doc: https://scrapeops.io/python-scrapy-playbook/scrapy-login-form/
- Youtube: https://www.youtube.com/watch?v=VySakHZi6HI&t=1s

## PhÃ¢n tÃ­ch trang Ä‘Äƒng nháº­p
1. PhÃ¢n tÃ­ch trang Ä‘Äƒng nháº­p
- URL mÃ  trang web cáº§n login (nhÆ° amazon cháº³ng háº¡n)
- payload cá»§a login prompt cáº§n send (form trong html)
- táº¥t cáº£ header vÃ  cookies mÃ  request cáº§n thiáº¿t Ä‘á»ƒ Ä‘Äƒng nháº­p trong tab Network.

2. Lá»±a chá»n method phÃ¹ há»£p Ä‘á»ƒ triá»ƒn khai dá»±a trÃªn nhá»¯ng gÃ¬ Ä‘Ã£ phÃ¢n tÃ­ch
- Method 1 - SimpleFormRequest: Hiáº¿m trang web nÃ o chá»‰ yÃªu cáº§u cÃ¡c thÃ´ng tin cÃ³ trong form html mÃ  thÆ°á»ng cÃ³ thÃªm cáº£ security token. Tuy nhiÃªn náº¿u trang web chá»‰ yÃªu cáº§u Ä‘Æ¡n giáº£n thÃ¬ chá»‰ cáº§n thay Scrapy request báº±ng FromRequest lÃ  Ä‘Æ°á»£c.
- Method 2 - FromRequestWithHidenData: ThÆ°á»ng sáº½ cÃ³ thÃªm tham sá»‘ phá»¥ ngoÃ i trong form cÃ³ type =hidden (tá»©c lÃ  ko tháº¥y trong ui). Tuy nhiÃªn náº¿u biáº¿t chÃ­nh xÃ¡c nÃ³ lÃ  gÃ¬ thÃ¬ chá»‰ cáº§n táº¡o form nhÆ° method 1.
- Method 3 - Headless Browser Logins: Má»™t vÃ i trang web Ä‘Äƒng nháº­p phá»©c táº¡p sáº½ chá»‰ generate hidden data trong js hoáº·c quÃ¡ trÃ¬nh Ä‘Äƒng nháº­p ráº¥t phá»©c táº¡p. LÃºc nÃ y Ä‘á»ƒ Ä‘Æ¡n giáº£n cÃ³ thá»ƒ sá»­ dá»¥ng headless server Ä‘á»ƒ login vÃ  tiáº¿p tá»¥c scrapy báº±ng nÃ³, hoáº·c chuyá»ƒn cookie cho scrapy request thÆ°á»ng.

## Method 1: SimpleFormRequest
1. Trong form Ä‘Äƒng nháº­p cÃ³ trÆ°á»ng csrf_token Ä‘Æ°á»£c áº©n, lÃ  access token. Sau Ä‘Ã³ lÃ  2 trÆ°á»ng user, password

# KhÃ³a há»c scrapy scale

## Nguá»“n
- doc: https://scrapeops.io/python-scrapy-playbook/scrapy-redis/
- youtube: https://www.youtube.com/watch?v=ZoosqkROKOI
## Táº¡i sao sá»­ dá»¥ng scrapy-redis
- Scalable: Giáº£m thá»i gian, chi phÃ­ crawl dá»¯ liá»‡u
- Reliable: Khi spider bá»‹ lá»—i, cÃ³ thá»ƒ start 1 worker khÃ¡c báº¯t Ä‘áº§u tá»« url bá»‹ lá»—i trÃªn redis
- Seperated data processing: dá»… dÃ ng phÃ¢n tÃ¡ch data processing pipeline cho cÃ¡c worker vÃ  scraping process.
- Syncronizing large crawls: lÃ m viá»‡c Ä‘a nhiá»‡m vá»›i nhiá»u spider cÃ¹ng 1 lÃºc.
## Kiáº¿n trÃºc phÃ¢n tÃ¡n
![](images/scrapy-redis.png)

Redis chá»©a cÃ¡c url cáº§n crawls, vai trÃ² nhÆ° 1 broker sá»­ dá»¥ng giao thá»©c MQTT.
## Kiáº¿n trÃºc phÃ¢n tÃ¡n boardcrawl
boardcrawl: thu tháº­p trÃªn diá»‡n rá»™ng, nhiá»u website cÃ¹ng 1 lÃºc.
- Váº¥n Ä‘á»: lÆ°á»£ng dá»¯ liá»‡u lá»›n vÃ  khÃ³ chia nhá» ngay tá»« Ä‘áº§u, scrapping job cÃ³ thá»ƒ tiÃªu thá»¥ nhiá»u tÃ i nguyÃªn, khÃ³ scale
- Scrapy-redis hoáº¡t Ä‘á»™ng: 
    - cÃ¡c crawler cÃ¹ng chia sáº» má»™t hÃ ng Ä‘á»£i redis
    - url má»›i phÃ¡t hiá»‡n Ä‘Æ°á»£c Ä‘áº©y vÃ o queue chung
    - cÃ¡c crawler láº¥y url tá»« queur Ä‘á»ƒ xá»­ lÃ½, trÃ¡nh bá»‹ trÃ¹ng láº·p.
    - náº¿u 1 crawler bá»‹ crash thÃ¬ cÃ´ng viá»‡c cÃ³ thá»ƒ tiáº¿p tá»¥c mÃ  ko máº¥t dá»¯ liá»‡u.
## Seperated Data Processing architech
Khi scraping large scale, nÃªn tÃ¡ch quÃ¡ trÃ¬nh trÃ­ch xuáº¥t HTML vá»›i quÃ¡ trÃ¬nh xá»­ lÃ½ data (item vÃ  pipeline). Äiá»u nÃ y giÃºp má»Ÿ rá»™ng quy mÃ´ thu  tháº­p dá»¯ liá»‡u Ä‘á»™c láº­p vá»›i nhau vÃ  giÃºp má»—i há»‡ thá»‘ng cÃ³ kháº£ nÄƒng chá»‹u lá»—i tá»‘t hÆ¡n.
LÃºc nÃ y cÃ³ thá»ƒ phÃ¢n tÃ¡n 2 láº§n: láº§n 1 Ä‘á»c url tá»« queue 1 vÃ  Ä‘áº©y queue 2, láº§n 2 Ä‘á»c queue 2 vÃ  xá»­ lÃ½ dá»¯ liá»‡u lÆ°u vÃ o DB.
![](images/scrapy-redis2.png)

## Thá»±c hÃ nh
- RedisSpider: láº¥y URL tá»« Redis vÃ  gá»i parse() Ä‘á»ƒ xá»­ lÃ½
- RedisCrawlSpider: sá»­ dá»¥ng rules Ä‘á»ƒ thu tháº­p cÃ¡c liÃªn káº¿t theo máº«u, dÃ¹ng khi url cáº§n truy cáº­p cÃ³ cáº¥u trÃºc rÃµ rÃ ng.
