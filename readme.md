# Nguá»“n
https://www.youtube.com/watch?v=mBoX_JCKZTE

# Æ¯u Ä‘iá»ƒm
request trá»±c tiáº¿p hoáº·c beautiful shop: phÃ¹ há»£p vá»›i crawl data má»™t láº§n, craw vÃ i page

scrapy: 
    tÃ­ch há»£p nhiá»u tÃ­nh nÄƒng há»— trá»£ cÃ o html, css, csv, json, xml, ... tá»‘t hÆ¡n.
    há»— trá»£ save vÃ o nhiá»u loáº¡i source nhÆ° S3, local
    tá»± Ä‘á»™ng quay láº¡i: náº¿u request láº§n 1 error, tá»± dá»™ng retry láº¡i.
    cÃ³ thá»ƒ trÃ­ch xuáº¥t 
    cÃ³ thá»ƒ

# Má»¥c lá»¥c

2. CÃ i Ä‘áº·t scrapy
3. Táº¡o scrapy project
4. Táº¡o spider
5. Táº¡o Spider negative qua cÃ¡c page khÃ¡c nhau 
6. LÃ m sáº¡ch dá»¯ liá»‡u quÃ¡ cÃ¡c pipeline nhá» hÆ¡n
7. LÆ°u data vÃ o Ä‘Ã¢u Ä‘Ã³ (file, database, s3, ...)
8. Fake scrapy headers vÃ  user-agents Ä‘á»ƒ bá» qua má»™t sá»‘ háº¡n cháº¿ vá» trÃ¬nh duyá»‡t trong má»™t sá»‘ website.
9. Rotating Proxies vÃ  Proxy APIS: áº©n IP Ä‘i 
10. Deploy spider vÃ  scheduler spiders báº±ng scrapyd
11. Deploy trong mÃ´i trÆ°á»ng production: ScrapeOps vÃ  Scrapy Cloud.

# Táº¡o project scrapy

scrapy startproject <tÃªn> => Táº¡o folder má»›i.

VD: scrapy startproject example

spiders: Chá»©a cÃ¡c spider.
middlewares files: LÃ  cÃ¡c file cÃ¹ng cáº¥p vá»›i thÆ° má»¥c spiders (thÆ°á»ng bá» qua). Tuy nhiÃªn file pipeline cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng 
    - settings.py: chá»©a cÃ¡c setting cá»§a project nhÆ° kÃ­ch hoáº¡t pipelines, middlewares, thá»i gian delay, concurrency (Ä‘á»“ng thá»i), robot.txt (cáº¥u hÃ¬nh scrapy)
    - items.py: model Ä‘á»ƒ extract data (CÃ¡c class entity)
    - pipelines.py: Thiáº¿t káº¿ pipelines tá»« nguá»“n vá» Ä‘Ã­ch (xá»­ lÃ½ dá»¯ liá»‡u sau crawl)
    - middlewares.py: há»¯u dá»¥ng Ä‘á»ƒ thá»±c hiá»‡n sá»­a Ä‘á»•i request vÃ  cÃ¡ch  scarpy chá»‰nh sá»­a reponse https
        time_out_request
        header muá»‘n gá»­i: user agents
        manage_cookie_cache: 
    - scrapy.cfg: cáº¥u hÃ¬nh scrapy

CÃ³ 2 loáº¡i middlewares: downloader middlewares (thÆ°á»ng hay Ä‘Æ°á»£c sá»­ dá»¥ng), spider middlewares. Chá»n cÃ¡i nÃ o thÃ¬ chá»‰nh trong setting.py

# Thuáº­t ngá»¯
1. feed: 1 dÃ²ng dá»¯ liá»‡u, ná»™i dung Ä‘Æ°á»£c cung cáº¥p tá»« soure vá» Ä‘Ã­ch (nhÆ° record)
    RSS feed: DÃ²ng tin tá»©c hoáº·c bÃ i bÃ¡o tá»« má»™t trang web
    Data feed: DÃ²ng dá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p tá»« má»™t há»‡ thá»‘ng (nhÆ° api hoáº·c db) cho á»©ng dá»¥ng khÃ¡c
    Feed Export: NÆ¡i dá»¯ liá»‡u Ä‘Æ°á»£c xuáº¥t ra (export). Trong scrapy há»— trá»£ feed export ra nhiá»u loáº¡i nhÆ° json, csv, ...

2. site = website

# Táº¡o spider
cd bookscraper
scrapy genspider bookspider books.toscrape.com

Truy cáº­p vÃ o scrapy shell báº±ng lá»‡nh scrapy shell (cÃ³ thá»ƒ thay shell máº·c Ä‘á»‹nh báº±ng ipython - nhÃ¢n jupyter)

# Lá»‡nh
1. scrapy list: liá»‡t kÃª cÃ¡c spider
2. scrapy crawl <tÃªn spider>: cháº¡y 1 con nhá»‡n
    option lÆ°u dá»¯ liá»‡u: 
    - thÃªm -O <tÃªn file><extention> Ä‘á»ƒ lÆ°u dá»¯ liá»‡u vÃ o 1 file nÃ o Ä‘Ã³, sáº½ ghi Ä‘Ã¨ náº¿u file Ä‘Ã£ tá»“n táº¡i
    - thÃªm -o <tÃªn file><extention> tÃ¡c dá»¥ng tÆ°Æ¡ng tá»± nhÆ°ng append vÃ o file náº¿u nÃ³ Ä‘Ã£ tá»“n táº¡i
    - sá»­ dá»¥ng file setting.py

# 8. Fake User-Agent and Brower headers
Má»¥c Ä‘Ã­ch: lÃ m cho con bot giá»‘ng nhÆ° Ä‘ang sá»­ dá»¥ng trÃ¬nh duyá»‡t tháº­t
User-agent: Chá»©a thÃ´ng tin cá»§a trÃ¬nh duyá»‡t, cáº§n pháº£i fake string nÃ y trong header cá»§a request Ä‘á»ƒ giáº£ dáº¡ng trÃ¬nh duyá»‡t tháº­t. 
1. Sá»­ dá»¥ng 1 trÃ¬nh duyá»‡t vá»›i má»i request (khÃ´ng khuyáº¿n khÃ­ch vÃ¬ cÃ³ thá»ƒ bá»‹ cháº·n)
    - Copy User-Agent String trong tab network: Báº­t F12 -> Network tab -> Refesh láº¡i trang Ä‘á»ƒ xem nhá»¯ng gÃ¬ mÃ  trÃ¬nh duyá»‡t Ä‘Ã£ request Ä‘áº¿n server vÃ  response cá»§a nÃ³, Ä‘á»c header vÃ  láº¥y
    - Äá»ƒ Ä‘á»c Ä‘Æ°á»£c thÃ´ng tin tá»« chuá»—i, truy cáº­p trang web [sau](https://useragentstring.com/) vÃ  Ä‘iá»n User-Agent vÃ o
2. Fake 1 user-agent má»¡i má»—i request
    - Ã tÆ°á»Ÿng táº¡o 1 list user/agent vÃ  loop qua nÃ³
    - Vá»›i cÃ¡c trang web lá»›n, cáº§n pháº£i táº¡o list cÃ³ hÃ ng ngÃ n user-agent. Hoáº·c sÆ° dá»¥ng Fake User Agent API (chá»‰ Ä‘á»‹nh trong file middlewares.py)
    - [API generator User-Agent](https://scrapeops.io/?fpr=lucas37&gad_source=1&gclid=CjwKCAiAzPy8BhBoEiwAbnM9O-yfrcSfS-0rgtjGz8fmFBTEUaem4rH0MVKA6jfvCX9qKh5MLY7j6hoCpD0QAvD_BwE): Cáº§n pháº£i táº¡o tÃ i khoáº£n (vohoang.w2002), scrapy cung cáº¥p free
3. Fake toÃ n bá»™ header
    - Fake user-agent lÃ  khÃ´ng Ä‘á»§ vá»›i cÃ¡c trang web lá»›n nhÆ° google, amazon, ... cáº§n pháº£i fake toÃ n bá»™ request header cho phÃ¹ há»£p vá»›i broswer.
    - Sá»­ dá»¥ng link api trÃªn cÃ³ thá»ƒ lÃ m Ä‘Æ°á»£c

# 9. Proxy
0. CÃ¡c trang web proxy free 
    - https://geonode.com/free-proxy-list
    - https://free-proxy-list.net/
1. Cho má»¥c Ä‘Ã­ch há»c táº­p, táº¡o mÃ¡y áº£o:
    - Forward Proxy (proxy thuáº­n - Ä‘áº¡i diá»‡n cho client): lÃ  mÃ¡y chá»§ trung gian giá»¯a client vÃ  internet. CÃ³ trÃ¡ch nhiá»‡m thay máº·t client gá»­i request Ä‘áº¿n báº¥t ká»³ Ä‘Ã¢u trong internet (router bgp cháº³ng háº¡n hay vpn). TÃ¡c dá»¥ng, che dáº¥Ãº ip cá»§a client (t,hÆ°á»ng ip client lÃ  private IP thÃ´ng qua bgp nÃªn cÅ©ng cháº³ng cáº§n che dáº¥u :v), kiá»ƒm soÃ¡t truy cáº­p, lá»c ná»™i dung, caching tÄƒng tá»‘c Ä‘á»™ truy cáº­p.
    ```sh
        sudo apt install squid -y
        sudo vi /etc/squid/squid.conf
        
        # acl localnet src 192.168.56.0/24   (cho phÃ©p cÃ¡c mÃ¡y thuá»™c dáº£i máº¡ng trÃªn sá»­ dá»¥ng proxy)
        # http_access allow localnet
        sudo systemctl restart squid

        # Test local, máº·c Ä‘á»‹nh squid chá»‰ há»— trá»£ http, cáº¥u hÃ¬nh https pháº£i thiáº¿t láº­p thÃªm
        curl -x http://192.168.56.161:3128 -I https://facebook.com
    ```
    - Reverse Proxy (proxy ngÆ°á»£c - Ä‘áº¡i diá»‡n cho server): lÃ  mÃ¡y chá»§ trung gian trong trÃ¬nh duyá»‡t web. Náº±m giá»¯a client vÃ  cÃ¡c server backend (che dáº¥u Ä‘Æ°á»£c ip server backend) vÃ  chuyá»ƒn tiáº¿p request cá»§a client Ä‘áº¿n server backend phÃ¹ há»£p (cÃ¢n báº±ng táº£i giá»¯a cÃ¡c server backend). TÃ¡c dá»¥ng cÃ¢n báº±ng táº£i, báº£o máº­t, caching, ssl termination (giáº£i mÃ£ SSL/TTS thay cho server backend).
    - Residential Proxies: 
2. Uptime lÃ  thá»i gian mÃ  má»™t dá»‹ch vá»¥ (nhÆ° proxy) hoáº¡t Ä‘á»™ng liÃªn tá»¥c mÃ  khÃ´ng bá»‹ giÃ¡n Ä‘oáº¡n. VÃ­ dá»¥, náº¿u má»™t proxy cÃ³ uptime 99%, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  trong 100 giá», proxy Ä‘Ã³ cÃ³ thá»ƒ sáº½ ngá»«ng hoáº¡t Ä‘á»™ng 1 giá». Uptime cÃ ng cao thÃ¬ proxy cÃ ng Ä‘Ã¡ng tin cáº­y vÃ  Ã­t bá»‹ giÃ¡n Ä‘oáº¡n.
3. Sá»­ dá»¥ng project cá»§a scrapy
    pip install scrapy-rotating-proxies
4. PhÆ°Æ¡ng phÃ¡p 1: Liá»‡t kÃª cÃ¡c proxy vÃ o file setting.py
5. PhÆ°Æ¡ng phÃ¡p 2: Liá»‡t kÃª cÃ¡c proxy vÃ o file proxies.txt vÃ  thÃªm Ä‘Æ°á»ng dáº«n cá»§a Ä‘áº¿n file Ä‘Ã³ vÃ o file setting.py
6. PhÆ°Æ¡ng phÃ¡p 3: Sá»­ dá»¥ng service cung cáº¥p ip vÃ  port cá»§a proxy hoáº¡t Ä‘á»™ng
    - 2 phÆ°Æ¡ng phÃ¡p trÃªn Ä‘ÃªÃ¹ cÃ³ nhÆ°á»£c Ä‘iá»ƒm lÃ  scrapy pháº£i check tá»«ng proxies hoáº¡t Ä‘á»™ng hay ko. LÃºc nÃ o cÅ©ng pháº£i lo láº¯ng vá» danh sÃ¡ch cÃ¡c proxies xem nÃ³ cÃ³ bá»‹ Ä‘á»•i ip hay ko, ...
    - service nÃ y Ä‘Æ°á»£c cung cáº¥p bá»Ÿi proxy provider vÃ  nÃ³ máº¥t phÃ­
    - Khi mua cáº§n pháº£i chÃº Ã½ location. Háº§u háº¿t cÃ¡c trang web sáº½ hoáº¡t Ä‘á»™ng khÃ¡c nhau khi á»Ÿ location khÃ¡c nhau.

Äi xa hÆ¡n, sá»­ dá»¥ng service cÃ³ phÃ­ cá»§a scrape ops, proxy tá»± Ä‘á»™ng cung cáº¥p dá»¯ liá»‡u trang web mÃ  ko cáº§n pháº£i fake user-agent hay sá»­ dá»¥ng proxy bÃªn thá»© 3 ná»¯a. Váº«n tÃ i khoáº£n Ä‘Ã³ trong scrape ops, Ä‘i vÃ o pháº§n proxy vÃ  xem video tá»«

# 10. Run spiders in cloud with scrapyd
1. Scrapyd - Free open source tool Ä‘á»ƒ cháº¡y nhiá»u spider trÃªn má»™t mÃ¡y chá»§ tá»« xa. 
    - LÃ  1 thÆ° viá»‡n python, cáº§n server tá»± táº¡o Ä‘á»ƒ cháº¡y
    - Chá»‰ cung cáº¥p api Ä‘á»ƒ thao tÃ¡c, giao diá»‡n cÃ³ thá»ƒ tÃ¹y chá»‰nh thÃ´ng qua pháº§n má»m cá»§a bÃªn thá»© 3.
    - no scheduler
    - CÃ¡ch sá»­ dá»¥ng
    ```sh
    # CÃ i Ä‘áº·t
    pip install scrapyd -y
    # Cháº¡y scrapyd á»Ÿ cháº¿ Ä‘á»™ backgroud
    scrapyd > scrapyd.log 2>&1 &
    # DÃ¹ng trÃ¬nh duyá»‡t hoáº·c curl Ä‘á»ƒ check
    curl http://localhost:6800/daemonstatus.json
    # CÃ i scrapyd client cho mÃ¡y chá»©a project vÃ  up nÃ³ lÃªn server.
    pip install git+https://github.com/scrapy/scrapyd-client.git
    # Äiá»u chá»‰nh trong file scrapy.cfg Ä‘á»ƒ chá»©a url cá»§a server (default lÃ  project name trong scrapyd)
    scrapy-deploy default
    # Project sáº½ tá»± sinh ra cÃ¡c thÆ° má»¥c nhÆ° build, project.egg-info
    # BÃ¢y giá» project Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai thÃ nh cÃ´ng lÃªn scrapy vÃ  sáºµn sÃ ng Ä‘á»ƒ cháº¡y
    curl http://localhost:6800/schedule.json -d project=bookscraper -d spider=bookspider
    # Dá»«ng scrapyd
    sudo ss -tunlp (láº¥y pid vÃ  kill nÃ³)
    kill <pid>
    ```

2. ScrapeOps - Free
    - Cáº§n cÃ³ server riÃªng tá»± táº¡o vÃ  cÃ i scrapyd
    - CÃ³ UI
    - CÃ³ kháº£ nÄƒng táº¡o jobs/spider vÃ  mornitor

3. ScrapyCloud - Paid

Triá»ƒn khai scrapyd
```sh
# TrÃªn mÃ¡y server scrapyd
git clone --depth 1 https://github.com/nkt780426/Learn-Scrapy.git
sudo apt install python3.10-venv -y
python3 -m venv crawl-data
source crawl-data/bin/activate
cd cd Learn-Scrapy/
pip install -r requirements.txt

cd Learn-Scrapy/crawl-data/lib/python3.10/site-packages/scrapyd/
vi default_scrapyd.conf # Sá»­a bind_address thÃ nh 0.0.0.0 Ä‘á»ƒ cháº¥p nháº­n client tá»« má»i nÆ¡i

# Cháº¡y scrapyd á»Ÿ cháº¿ Ä‘á»™ background vÃ  ghi log vÃ o file scrapyd.log thay vÃ¬ in ra terminal
export SCRAPYD_BIND_ADDRESS=0.0.0.0
scrapyd > scrapyd.log 2>&1 &

# Kiá»ƒm tra status cá»§a scrapyd server tá»« mÃ¡y client
(base) user3t@LAP027:~$ curl http://192.168.56.160:6800/daemonstatus.json
{"pending": 0, "running": 0, "finished": 0, "status": "ok", "node_name": "scrapyd"} # KhÃ´ng cÃ³ job nÃ o pedding, running, finished

# Package spider vÃ  up lÃªn srapyd server
# Chá»‰nh url cá»§a server trong file scrapy.cfg
# Theo máº·c Ä‘á»‹nh scrapy chá»‰ Ä‘Ã³ng gÃ³i cÃ¡c tá»‡p scrapy lÃªn server. Muá»‘n nÃ³ cÃ³ thÃªm file proxies.txt cáº§n pháº£i config trong setup.py
# Push spider lÃªn server (nhÆ° git)
scrapyd-deploy default
(craw_data) (base) user3t@LAP027:~/Workspace/projects/in-process/scrapy/bookscraper$ scrapyd-deploy default
Packing version 1738723310
Deploying to project "bookscraper" in http://192.168.56.160:6800/addversion.json
Server response (200):
{"project": "bookscraper", "version": "1738723310", "spiders": 1, "status": "ok", "node_name": "scrapyd"} # Project name, version push, sá»‘ lÆ°á»£ng spider trong project, status push, hostnamectl cá»§a server.
# Scheduler spider
curl http://192.168.56.160:6800/schedule.json -d project=bookscraper -d spider=bookspider
# Kiá»ƒm tra job cÃ³ cháº¡y ko
curl http://192.168.56.160:6800/listjobs.json?project=bookscraper
# Kiá»ƒm tra log
curl http://192.168.56.160:6800/logs/bookscraper/bookspider/af830432e36c11efaf13a10e0aa0ee42.log
```
Triá»ƒn khai scrapydweb
```sh
pip install -r requirements.txt
# Láº§n Ä‘áº§u tiÃªn cháº¡y sáº½ sinh ra file scrapydweb_settings_v10.py á»Ÿ ~
scrapydweb
# Äiá»u chá»‰nh cáº¥u hÃ¬nh thÃ nh
SCRAPYD_SERVERS = [
    '127.0.0.1:6800'
]
ENABLE_LOGPARSER = True 
LOCAL_SCRAPYD_SERVER = '127.0.0.1:6800'
LOCAL_SCRAPYD_LOGS_DIR = '/home/vohoang/scrapyd/logs'
# Dá»«ng scrapyd vÃ  start láº¡i
mkdir scrapyd
cd scrapyd 
scrapyd > scrapyd.log 2>&1 &
# Cháº¡y láº¡i vÃ  táº­n hÆ°á»Ÿng á»Ÿ port 5000 (flask)
scrapydweb > scrapydweb.log 2>&1 &
```

# 13. Recap
1. CÃ¡c váº¥n Ä‘á» cÃ²n tá»“n Ä‘á»ng
    - dynamic website: ná»™i dung sinh ra chá»‰ khi di chuyá»ƒn view, front-end framework chá»‰ hiá»ƒn thá»‹ 1 pháº§n thÃ´ng tin (1 page) mÃ  server gá»­i. Do Ä‘Ã³ náº¿u sá»­ dá»¥ng url tá»« website nÃ y sáº½ khÃ´ng nháº­n Ä‘Æ°á»£c data. => Scrapy pupeteer hoáº·c scrapy selenium
```sh
# dÃ¹ng Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u tá»« cÃ¡c trang web sá»­ dá»¥ng JavaScript Ä‘á»™ng nhÆ° SPA - Single Page Application. Puppeteer giÃºp render hoÃ n chá»‰nh trang web giÃºp scrapy láº¥y Ä‘Æ°á»£c dá»¯ liá»‡u sau khi JS cháº¡y xong
pip install scrapy-puppeteer
# ThÃªm middleware vÃ o puppeteer
DOWNLOADER_MIDDLEWARES = {
    'scrapy_puppeteer.middleware.PuppeteerMiddleware': 800,
}
# Táº¡o spider
import scrapy
from scrapy_puppeteer import PuppeteerRequest

class MySpider(scrapy.Spider):
    name = 'myspider'
    start_urls = ['https://example.com']

    async def parse(self, response):
        yield PuppeteerRequest(
            url='https://example.com',
            callback=self.parse_result
        )

    async def parse_result(self, response):
        title = response.css('title::text').get()
        yield {'title': title}
# DÃ¹ng khi trang web sá»­ dá»¥ng nhiá»u js Ä‘á»ƒ táº£i ná»™i dung hay ko thá»ƒ láº¥y Ä‘Æ°á»£c dá»¯ liá»‡u vÃ¬ ná»™i dung khÃ´ng cÃ³ trong html ban Ä‘áº§u, cáº§n tÆ°Æ¡ng tÃ¡c vá»›i trang web nhÆ° click, scroll, Ä‘iá»n form, ...
# Náº¿u chá»‰ cáº§n láº¥y dá»¯ liá»‡u tá»« API áº©n trong trang thÃ¬ cÃ³ thá»ƒ thá»­ Scrapy + request vÃ¬ puppeteer cháº­m hÆ¡n.
```
```sh
# scrapy-selenium cÅ©ng lÃ  middleware há»— trá»£ crawl dynamic web
# selenium sá»­ dá»¥ng webdriver thay vÃ¬ puppeteer
# DÃ¹ng khi cáº§n tÆ°Æ¡ng tÃ¡c vá»›i web nhÆ° click, scroll, Ä‘iá»n form, Ä‘Äƒng nháº­p, ...
pip install scrapy-selenium
# CÃ i thÃªm web driver ná»¯a
DOWNLOADER_MIDDLEWARES = {
    'scrapy_selenium.SeleniumMiddleware': 800
}

SELENIUM_DRIVER_NAME = 'chrome'  # Hoáº·c 'firefox'
SELENIUM_DRIVER_EXECUTABLE_PATH = '/path/to/chromedriver'  # ÄÆ°á»ng dáº«n Ä‘áº¿n WebDriver
SELENIUM_DRIVER_ARGUMENTS=['--headless']  # Cháº¡y khÃ´ng hiá»ƒn thá»‹ trÃ¬nh duyá»‡t
# VÃ­ dá»¥ Ä‘Æ¡n giáº£n
import scrapy
from scrapy_selenium import SeleniumRequest

class MySpider(scrapy.Spider):
    name = "my_spider"

    def start_requests(self):
        yield SeleniumRequest(
            url="https://example.com",
            callback=self.parse
        )

    def parse(self, response):
        title = response.css('title::text').get()
        yield {"title": title}
```

| ğŸ›  CÃ´ng cá»¥         | ğŸŒ TrÃ¬nh duyá»‡t                  | ğŸš€ Hiá»‡u suáº¥t  | ğŸ”§ Khi nÃ o nÃªn dÃ¹ng?                                      |
|-------------------|--------------------------------|--------------|------------------------------------------------------|
| **Scrapy-Selenium**  | Chrome, Firefox, Edge, Safari... | Cháº­m hÆ¡n     | Khi cáº§n tÆ°Æ¡ng tÃ¡c vá»›i trang web (click, form...)     |
| **Scrapy-Puppeteer** | Chá»‰ há»— trá»£ Chrome/Chromium    | Nhanh hÆ¡n    | Khi chá»‰ cáº§n render JavaScript mÃ  khÃ´ng cáº§n nhiá»u tÆ°Æ¡ng tÃ¡c |

Sá»­ dá»¥ng 2 cÃ´ng cá»¥ trÃªn sáº½ cháº¡y vá»›i headless broswer (khÃ´ng cáº§n ui)

2. Login endpoint
3. Scale scrape (náº¿u pháº£i cÃ o 1000 page 1 ngÃ y thÃ¬ sao)
    - Sá»­ dá»¥ng database Ä‘á»ƒ lÆ°u cÃ¡c url cáº§n xá»­ lÃ½, sau Ä‘Ã³ dÃ¹ng nhiá»u server

Äá»c link mÃ  khÃ³a há»c recommend. scrapy-playwright Ä‘Ã£ thay tháº¿ scrapy-ppuppeteer
https://thepythonscrapyplaybook.com/freecodecamp-beginner-course/freecodecamp-scrapy-beginners-course-part-13-next-steps/